= Public HTTPS Endpoints =
:toc:
 
== Introduction ==
 
The directory contains replica components that open active network sockets, so NNS managed nodes can 
communicate with each other.
 

The replica components in this directory are:

* link:public/README.adoc[public HTTPS API endpoint], serving incoming requests from boundary nodes and other replica nodes
* metrics HTTPS endpoint, used by https://prometheus.io/[Prometheus] for scraping

== Connection management ==

Components in scope assume persistent connections are established. A hard cap exists on
the number of live TCP connections. If the limit is reached, new incoming connections are refused.
Under extreme load, when the rate of incoming connections is higher than the rate at which the
replica accepts them, users may also observe refused connections due to a full TCP listener
backlog queue.

Given the existence of persistent connections, we must detect dead peers, disconnections due
to network inactivity and/or peers holding on to connections without sending requests.
For this purpose, if no bytes are read from a connection for the duration of 
'connection_read_timeout_seconds', defined in each component's config, then the connection is
dropped. There is no point in setting a timeout on the write bytes since they are conditioned
on the received requests. 

Each component uses the ReplicaOS defaults for https://tldp.org/HOWTO/TCP-Keepalive-HOWTO/overview.html#whyuse[TCP alivekeep].

== Thread-per-request ==
 
Components in scope listen on a socket, parse the corresponding request and execute the requested API
call by calling an upstream component(s) that may block the running thread. Since async code needs to be
https://docs.rs/tokio/latest/tokio/task/index.html[non-blocking], the components use 
https://sre.google/sre-book/addressing-cascading-failures/#xref_cascading-failure_queue-management[thread-per-request]
pattern for executing the corresponding upstream API calls. More specifically, there is a
queue in front of each thread pool that handles requests. Requests come in, they sit in a bounded-size queue, and then
threads pick requests off the queue and perform the actual work (whatever actions are required by the replica).
The caller is responsible for correctly handling the case when the queue is full.

If a request is cancelled before it is picked up from a thread, then the request is not executed.

This design pattern is implemented using a https://docs.rs/tower/latest/tower/limit/concurrency/index.html[limit on number of requests being concurrently processed],
https://docs.rs/threadpool/latest/threadpool/[threadpool] per upstream service and https://docs.rs/tokio/latest/tokio/sync/oneshot/index.html[Tokio oneshot channel].

== Request timeout ==

In order to guard against stuck upstream services, a https://docs.rs/tower/latest/tower/timeout/index.html[timeout] is set for each received request. 
If a request is not completed within the timeout then the endpoint responds with `+504 Gateway Timeout+`.

Setting a timeout on requests also guards against closing an idle connection due to stuck upstream service.
For example, if a user uses HTTP1 over a connection and there is a upstream service that takes longer
than it should, then the user is unable to send new requests until the last one completed.
Hence, this may result in dropping the connection because no new bytes are read by the replica on that
connection. 

== Preventing Server Overload ==
 
Servers should protect themselves from becoming overloaded and crashing. When overloaded at either the frontend or
backend layers, fail early and cheaply. For details, see 
https://sre.google/sre-book/addressing-cascading-failures/#xref_cascading-failure_load-shed-graceful-degredation[Load Shedding and Graceful Degradation.]

In addition, serving errors early and cheaply can be beneficial for replicated servers that stay behind load balancers.
For example, https://sre.google/sre-book/load-balancing-datacenter/[Least-Loaded Round Robin] takes into account recent errors.
 
Given the listed best practices, when a particular queue in-front of a thread pool is full and request can't be added,
the endpoint https://docs.rs/tower/latest/tower/load_shed/index.html#[sheds load] by responding with `+429 Too Many Requests+` for that particular request. 

== Too large and too slow requests ==

If a http request body is greater than the configured limit, the endpoints responds with `+413 Payload Too Large+`.

If a http request does not complete within the specified timeout it will be aborted and a `+408 Request Timeout+` response will be sent.

== Fairness ==

Fairness implies that the rate at which requests from different connections are picked up is more or less the same.

Currently boundary nodes use https://www.nginx.com/[nginx] as reverse proxy. Thus, the boundary nodes
can only use https://mailman.nginx.org/pipermail/nginx/2015-December/049445.html[HTTP1] for communicating with replicas.
Hence, there can be at most one in-flight request in the replica from each connection. Given we use https://tokio.rs/blog/2019-10-scheduler[Tokio's fair scheduler]
and we have a dedicated Tokio task for serving each connection this is sufficient for request accross connections.

