[sources.vector_metrics]
type = "internal_metrics"

[sinks.vector_exporter]
type = "prometheus_exporter"
inputs = ["vector_metrics"]
address = "${VECTOR_PROMETHUS_ADDR}"
default_namespace = "vector"

# nginx

[sources.nginx]
type = "journald"
include_units = ["nginx"]

# nginx access

[transforms.nginx_access]
type = "filter"
inputs = ["nginx"]
condition = ".SYSLOG_IDENTIFIER == \"access\""

# Do not index remote_user, remote_addr and remote_port
[transforms.nginx_access_json]
type = "remap"
inputs = ["nginx_access"]
source = """
. = parse_json!(.message)

del(.remote_user)
del(.remote_addr)
del(.remote_port)

.@timestamp, err = to_float(.msec) * 1000
if err != null {
  .@timestamp = null
}

.@timestamp = to_int(.@timestamp)
if .@timestamp == 0 {
  .@timestamp = null
}

status, err = to_int(.status)
if err == null {
  .status = status
}

for_each([
  "ic_subnet_id",
  "ic_node_id",
  "ic_canister_id",
]) -> |_, k| {
  if get!(., [k]) == "" {
    . = set!(., [k], "N/A")
  }
}
"""

# nginx access (elasticsearch)

[transforms.nginx_access_by_status]
type = "route"
inputs = [ "nginx_access_json" ]

  [transforms.nginx_access_by_status.route]
  1xx = '.status >= 100 && .status < 200 ?? false'
  2xx = '.status >= 200 && .status < 300 ?? false'
  3xx = '.status >= 300 && .status < 400 ?? false'
  4xx = '.status >= 400 && .status < 500 ?? false'
  5xx = '.status >= 500 && .status < 600 ?? false'

[transforms.nginx_access_2xx_sampled]
type = "sample"
inputs = [ "nginx_access_by_status.2xx" ]
rate = 100

[transforms.nginx_access_tag]
type = "remap"
inputs = [
  "nginx_access_by_status.1xx",
  "nginx_access_2xx_sampled",
  "nginx_access_by_status.3xx",
  "nginx_access_by_status.4xx",
  "nginx_access_by_status.5xx",
  "nginx_access_by_status._unmatched",
]
source = """
tags = [ string(.tags || "") ?? "", "${ELASTICSEARCH_TAGS}" ]
tags = filter(tags) -> |_index, value| { value != "" }
tags, err = join(tags, ", ")
if err == null && length(tags) != 0 {
  .tags = tags
}
"""

[sinks.elasticsearch_nginx_access]
type = "elasticsearch"
inputs = [ "nginx_access_tag" ]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"

  [sinks.elasticsearch_nginx_access.bulk]
  index = "boundary-node-nginx-access-%Y.%m.%d"

  [sinks.elasticsearch_nginx_access.tls]
  verify_certificate = false

# nginx access (metrics)

[transforms.nginx_access_metrics]
type = "log_to_metric"
inputs = ["nginx_access_json"]

  [[transforms.nginx_access_metrics.metrics]]
  type = "counter"
  field = "status"
  name = "request_total"

    [transforms.nginx_access_metrics.metrics.tags]
    hostname = "{{ hostname }}"
    ic_node_id = "{{ ic_node_id }}"
    ic_subnet_id = "{{ ic_subnet_id }}"
    request_method = "{{ request_method }}"
    status = "{{ status }}"
    traffic_segment = "{{ traffic_segment }}"
    is_bot = "{{ is_bot }}"
    upstream_cache_status = "{{ upstream_cache_status }}"
    upstream_status = "{{ upstream_status }}"

  [[transforms.nginx_access_metrics.metrics]]
  type = "histogram"
  field = "request_time"
  name = "request_sec_duration"

    [transforms.nginx_access_metrics.metrics.tags]
    hostname = "{{ hostname }}"
    ic_node_id = "{{ ic_node_id }}"
    ic_subnet_id = "{{ ic_subnet_id }}"
    request_method = "{{ request_method }}"
    status = "{{ status }}"
    traffic_segment = "{{ traffic_segment }}"
    is_bot = "{{ is_bot }}"
    upstream_cache_status = "{{ upstream_cache_status }}"
    upstream_status = "{{ upstream_status }}"

# nginx error

[transforms.nginx_error]
type = "filter"
inputs = ["nginx"]
condition = ".SYSLOG_IDENTIFIER == \"error\""

[transforms.nginx_error_clean]
type = "filter"
inputs = ["nginx_error"]
condition = "!contains(string!(.message), \"closed keepalive connection\")"

[transforms.nginx_error_json]
type = "remap"
inputs = ["nginx_error_clean"]
source = """
.@timestamp, err = to_int(.__REALTIME_TIMESTAMP)
if err != null {
  .@timestamp = null
}

.@timestamp, err = .@timestamp / 1000
if err != null {
  .timestamp = null
}
.@timestamp = to_int(.@timestamp)

. = {
  "@timestamp": .@timestamp,
  "host": .host,
  "message": .message
}
"""

# nginx error (metrics)

[transforms.nginx_error_metrics]
type = "log_to_metric"
inputs = ["nginx_error_json"]

  [[transforms.nginx_error_metrics.metrics]]
  type = "counter"
  field = "message"
  name = "error_total"

    [transforms.nginx_error_metrics.metrics.tags]
    hostname = "{{ host }}"

# nginx (prometheus)

[sinks.prometheus_exporter_nginx]
type = "prometheus_exporter"
inputs = ["nginx_access_metrics", "nginx_error_metrics"]
address = "${NGINX_PROMETHUS_ADDR}"
default_namespace = "nginx"

# control-plane

[sources.control_plane]
type = "journald"
include_units = ["control-plane"]

[transforms.control_plane_json]
type = "remap"
inputs = ["control_plane"]
source = """
preserved_fields = {}; preserved_keys = ["host"]

for_each(preserved_keys) -> |_, k| {
  v = get!(., [k])
  if v != null {
    preserved_fields = set!(preserved_fields, [k], v)
  }
}

. = parse_json!(.message)

.@timestamp = to_timestamp!(.timestamp, unit: "milliseconds")
.@timestamp = to_unix_timestamp(.@timestamp, unit: "milliseconds")
del(.timestamp)

. = merge(., preserved_fields)
"""

[sinks.elasticsearch_control_plane]
type = "elasticsearch"
inputs = ["control_plane_json"]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"

  [sinks.elasticsearch_control_plane.bulk]
  index = "boundary-node-control-plane-%Y.%m.%d"

  [sinks.elasticsearch_control_plane.tls]
  verify_certificate = false

# certificate-issuer

[sources.certificate_issuer]
type = "journald"
include_units = ["certificate-issuer"]

[transforms.certificate_issuer_json]
type = "remap"
inputs = ["certificate_issuer"]
source = """
preserved_fields = {}; preserved_keys = ["host"]

for_each(preserved_keys) -> |_, k| {
  v = get!(., [k])
  if v != null {
    preserved_fields = set!(preserved_fields, [k], v)
  }
}

. = parse_json!(.message)

.@timestamp = to_timestamp!(.timestamp, unit: "milliseconds")
.@timestamp = to_unix_timestamp(.@timestamp, unit: "milliseconds")
del(.timestamp)

. = merge(., preserved_fields)
"""

[sinks.elasticsearch_certificate_issuer]
type = "elasticsearch"
inputs = ["certificate_issuer_json"]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"

  [sinks.elasticsearch_certificate_issuer.bulk]
  index = "boundary-node-certificate-issuer-%Y.%m.%d"

  [sinks.elasticsearch_certificate_issuer.tls]
  verify_certificate = false

# certificate-syncer

[sources.certificate_syncer]
type = "journald"
include_units = ["certificate-syncer"]

[transforms.certificate_syncer_json]
type = "remap"
inputs = ["certificate_syncer"]
source = """
preserved_fields = {}; preserved_keys = ["host"]

for_each(preserved_keys) -> |_, k| {
  v = get!(., [k])
  if v != null {
    preserved_fields = set!(preserved_fields, [k], v)
  }
}

. = parse_json!(.message)

.@timestamp = to_timestamp!(.timestamp, unit: "milliseconds")
.@timestamp = to_unix_timestamp(.@timestamp, unit: "milliseconds")
del(.timestamp)

. = merge(., preserved_fields)
"""

[sinks.elasticsearch_certificate_syncer]
type = "elasticsearch"
inputs = ["certificate_syncer_json"]
endpoint = "${ELASTICSEARCH_URL}"
mode = "bulk"

  [sinks.elasticsearch_certificate_syncer.bulk]
  index = "boundary-node-certificate-syncer-%Y.%m.%d"

  [sinks.elasticsearch_certificate_syncer.tls]
  verify_certificate = false

# danted (socks proxy)

[sources.danted]
type = "journald"
include_units = ["danted"]

[transforms.danted_json]
type = "remap"
inputs = ["danted"]
source = """
preserved_fields = {}; preserved_keys = ["host", "timestamp"]

for_each(preserved_keys) -> |_, k| {
  v = get!(., [k])
  if v != null {
    preserved_fields = set!(preserved_fields, [k], v)
  }
}

.message = string!(.message)
addrs = split(.message, " [: ")[-1]
addrs = split(string!(addrs), " ")

client_addr_with_port = split(addrs[0], ".") ?? ["N/A", "N/A"]
server_addr_with_port = split(addrs[1], ".") ?? ["N/A", "N/A"]

. = merge({
  "client_addr": client_addr_with_port[0],
  "client_port": client_addr_with_port[1],
  "server_addr": server_addr_with_port[0],
  "server_port": server_addr_with_port[1],
}, preserved_fields)
"""

# danted (socks proxy) (metrics)

[transforms.danted_metrics]
type = "log_to_metric"
inputs = ["danted_json"]

  [[transforms.danted_metrics.metrics]]
  type = "counter"
  field = "timestamp"
  name = "requests_total"

    [transforms.danted_metrics.metrics.tags]
    hostname = "{{ host }}"
    client_addr = "{{ client_addr }}"
    server_addr = "{{ server_addr }}"

# danted (socks proxy) (prometheus)

[sinks.prometheus_exporter_danted]
type = "prometheus_exporter"
inputs = ["danted_metrics"]
address = "${DANTED_PROMETHUS_ADDR}"
default_namespace = "danted"
